{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-aRiOgl4nHg"
   },
   "source": [
    "------\n",
    "**You cannot save any changes you make to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
    "\n",
    "------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIZrAUx57vsM"
   },
   "source": [
    "Practical 1: Sentiment Detection in Movie Reviews\n",
    "========================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4kXPMhyngZW"
   },
   "source": [
    "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
    "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
    "Each review is a **document** and consists of one or more sentences.\n",
    "\n",
    "To prepare yourself for this practical, you should\n",
    "have a look at a few of these texts to understand the difficulties of\n",
    "the task: how might one go about classifying the texts? You will write\n",
    "code that decides whether a movie review conveys positive or\n",
    "negative sentiment.\n",
    "\n",
    "Please make sure you have read the following paper:\n",
    "\n",
    ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
    "(2002).\n",
    "[Thumbs up? Sentiment Classification using Machine Learning\n",
    "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
    "\n",
    "Bo Pang et al. introduced the movie review sentiment\n",
    "classification task, and the above paper was one of the first papers on\n",
    "the topic. The first version of your sentiment classifier will do\n",
    "something similar to Pang et al.'s system. If you have questions about it,\n",
    "you should resolve you doubts as soon as possible with your TA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb7errgRASzZ"
   },
   "source": [
    "**Advice**\n",
    "\n",
    "Please read through the entire practical and familiarise\n",
    "yourself with all requirements before you start coding or otherwise\n",
    "solving the tasks. Writing clean and concise code can make the difference\n",
    "between solving the assignment in a matter of hours, and taking days to\n",
    "run all experiments.\n",
    "\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "While we inserted code cells to indicate where you should implement your own code, please feel free to add/remove code blocks where you see fit (but make sure that the general structure of the assignment is preserved). Also, please keep in mind that it is always good practice to structure your code properly, e.g., by implementing separate classes and functions that can be reused. **Make sure you run all your code before submitting the notebook, and do not leave unnecessary print statements / cells in your notebook that are not intended for the grader.**\n",
    "\n",
    "## Environment\n",
    "\n",
    "All code should be written in **Python 3**.\n",
    "This is the default in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1698047665308,
     "user": {
      "displayName": "XUE LI",
      "userId": "02386889076941396043"
     },
     "user_tz": -120
    },
    "id": "SaZnxptMJiD7",
    "outputId": "18c3a760-90b6-4d90-e507-aba57841ae23"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYZyIF7lJnGn"
   },
   "source": [
    "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
    "The easiest way to\n",
    "install Python is through downloading\n",
    "[Anaconda](https://www.anaconda.com/download).\n",
    "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
    "You can also use an IDE\n",
    "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
    "coding and debugging easier. It is good practice to create a [virtual\n",
    "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
    "project, so that any Python packages don’t interfere with other\n",
    "projects.\n",
    "\n",
    "\n",
    "**Learning Python 3**\n",
    "\n",
    "If you are new to Python 3, you may want to check out a few of these resources:\n",
    "- https://learnxinyminutes.com/docs/python3/\n",
    "- https://www.learnpython.org/\n",
    "- https://docs.python.org/3/tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "hok-BFu9lGoK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "from subprocess import call\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import sklearn as sk\n",
    "#from google.colab import drive\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXWyGHwE-ieQ"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "**Download the sentiment lexicon and the movie reviews dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1808,
     "status": "ok",
     "timestamp": 1698047686292,
     "user": {
      "displayName": "XUE LI",
      "userId": "02386889076941396043"
     },
     "user_tz": -120
    },
    "id": "lm-rakqtlMOT",
    "outputId": "1b6b1152-50b7-4161-aab2-03da318d6981"
   },
   "outputs": [],
   "source": [
    "# download sentiment lexicon\n",
    "!wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
    "# download review data\n",
    "!wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkPwuHp5LSuQ"
   },
   "source": [
    "**Load the movie reviews.**\n",
    "\n",
    "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3917,
     "status": "ok",
     "timestamp": 1698047715771,
     "user": {
      "displayName": "XUE LI",
      "userId": "02386889076941396043"
     },
     "user_tz": -120
    },
    "id": "careEKj-mRpl",
    "outputId": "a6f3c385-5106-49ec-be89-4aff971eb05b"
   },
   "outputs": [],
   "source": [
    "# file structure:\n",
    "# [\n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
    "#   ..\n",
    "# ]\n",
    "# where `content` is a list of sentences,\n",
    "# with a sentence being a list of (token, pos_tag) pairs.\n",
    "\n",
    "\n",
    "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "  reviews = json.load(f)\n",
    "\n",
    "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
    "\n",
    "def print_sentence_with_pos(s):\n",
    "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
    "\n",
    "for i, r in enumerate(reviews):\n",
    "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
    "  print_sentence_with_pos(r[\"content\"][0])\n",
    "  if i == 4:\n",
    "    break\n",
    "\n",
    "c = Counter()\n",
    "for review in reviews:\n",
    "  for sentence in review[\"content\"]:\n",
    "    for token, pos_tag in sentence:\n",
    "      c[token.lower()] += 1\n",
    "\n",
    "print(\"\\nNumber of word types:\", len(c))\n",
    "print(\"Number of word tokens:\", sum(c.values()))\n",
    "\n",
    "print(\"\\nMost common tokens:\")\n",
    "for token, count in c.most_common(20):\n",
    "  print(\"%10s : %8d\" % (token, count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6PWaEoh8B34"
   },
   "source": [
    "#(1) Lexicon-based approach (3.5pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsTSMb6ma4E8"
   },
   "source": [
    "A traditional approach to classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative, or a score from 0 to 5).\n",
    "\n",
    "In this practical, you will use the sentiment\n",
    "lexicon released by Wilson et al. (2005).\n",
    "\n",
    "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
    "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
    "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
    "\n",
    "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. The path of the lexicon file is `\"sent_lexicon\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1698047764860,
     "user": {
      "displayName": "XUE LI",
      "userId": "02386889076941396043"
     },
     "user_tz": -120
    },
    "id": "Ogq0Eq2hQglh",
    "outputId": "1c26526d-399e-4558-a1b2-2fa771648f09"
   },
   "outputs": [],
   "source": [
    "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "  line_cnt = 0\n",
    "  for line in f:\n",
    "    print(line.strip())\n",
    "    line_cnt += 1\n",
    "    if line_cnt > 4:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mml4nOtIUBhn"
   },
   "source": [
    "Lexica such as this can be used to solve\n",
    "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
    "$S_{binary}$ by counting how many words have a positive or a\n",
    "negative label in the sentiment lexicon $SLex$.\n",
    "\n",
    "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
    "\n",
    "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
    "\n",
    "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
    "\n",
    "$$\n",
    "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
    "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
    "        \\text{negative} & \\text{otherwise}\n",
    "        \\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOFnMvbeeZrc"
   },
   "source": [
    "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "ED2aTEYutW1-"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def get_binary_lexicon_pred(review, lexicon: dict, threshold: float = 8) -> str:\n",
    "    score = 0\n",
    "    review_tokens = []\n",
    "    for sentence in review:\n",
    "        review_tokens += [token for token, _ in sentence]\n",
    "    for token in review_tokens:\n",
    "        if token in lexicon:\n",
    "            if lexicon[token][\"polarity\"] == \"positive\":\n",
    "                score += 1\n",
    "            elif lexicon[token][\"polarity\"] == \"negative\":\n",
    "                score -= 1\n",
    "    if score > threshold:\n",
    "        return \"POS\", score\n",
    "    else:\n",
    "        return \"NEG\", score\n",
    "    \n",
    "lexicon_dict = {}\n",
    "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        lexicon_entry = line.strip().split()\n",
    "        word = lexicon_entry[2].split(\"=\")[1]\n",
    "        polarity = lexicon_entry[5].split(\"=\")[1]\n",
    "        magnitude = lexicon_entry[0].split(\"=\")[1]\n",
    "        lexicon_dict[word] = {\n",
    "            \"polarity\": polarity,\n",
    "            \"magnitude\": magnitude,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary = []\n",
    "for r in reviews:\n",
    "    review = r[\"content\"]\n",
    "    y_binary.append(get_binary_lexicon_pred(review, lexicon_dict)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iy528EUTphz5"
   },
   "outputs": [],
   "source": [
    "# token_results should be a list of binary indicators; for example [1, 0, 1, ...]\n",
    "# where 1 indicates a correct classification and 0 an incorrect classification.\n",
    "token_results = [1 if y == y_binary[i] else 0 for i, y in enumerate([r[\"sentiment\"] for r in reviews])]\n",
    "token_accuracy = sum(token_results) / len(token_results)\n",
    "print(f\"Accuracy: {token_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Twox0s_3eS0V"
   },
   "source": [
    "As the sentiment lexicon also has information about the **magnitude** of\n",
    "sentiment (e.g., *“excellent\"* has the same sentiment _polarity_ as *“good\"* but it has a higher magnitude), we can take a more fine-grained approach by adding up all\n",
    "sentiment scores, and deciding the polarity of the movie review using\n",
    "the sign of the weighted score $S_{weighted}$.\n",
    "\n",
    "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
    "\n",
    "\n",
    "Make sure you define an appropriate threshold for this approach.\n",
    "\n",
    "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "qG3hUDnPtkhS"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def get_weighted_lexicon_pred(\n",
    "    review: str, lexicon: dict, threshold: float = 0\n",
    ") -> tuple[str, int]:\n",
    "    score = 0\n",
    "    review_tokens = []\n",
    "    for sentence in review:\n",
    "        review_tokens += [token for token, _ in sentence]\n",
    "    for token in review_tokens:\n",
    "        if token in lexicon:\n",
    "            polarity = lexicon[token][\"polarity\"]\n",
    "\n",
    "            # Transform polarity and magnitude to numerical values\n",
    "            if polarity == \"positive\":\n",
    "                polarity_value = 1\n",
    "            elif polarity == \"negative\":\n",
    "                polarity_value = -1\n",
    "            else:\n",
    "                polarity_value = 0\n",
    "\n",
    "            magnitude = lexicon[token][\"magnitude\"]\n",
    "            if magnitude == \"weaksubj\":\n",
    "                magnitude_value = 1\n",
    "            elif magnitude == \"strongsubj\":\n",
    "                magnitude_value = 2\n",
    "\n",
    "            score += polarity_value * magnitude_value\n",
    "\n",
    "    if score > threshold:\n",
    "        return \"POS\", score\n",
    "    else:\n",
    "        return \"NEG\", score\n",
    "\n",
    "\n",
    "y_magnitude = []\n",
    "scores = []\n",
    "for r in reviews:\n",
    "    review = r[\"content\"]\n",
    "    pred, score = get_weighted_lexicon_pred(review, lexicon_dict)\n",
    "    y_magnitude.append(pred)\n",
    "    scores.append(score)\n",
    "\n",
    "scores = np.array(scores)\n",
    "\n",
    "token_results_weighted = [\n",
    "    1 if y == y_magnitude[i] else 0\n",
    "    for i, y in enumerate([r[\"sentiment\"] for r in reviews])\n",
    "]\n",
    "token_accuracy_weighted = sum(token_results_weighted) / len(token_results_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_threshold = scores.mean()\n",
    "y_magnitude = []\n",
    "scores = []\n",
    "for r in reviews:\n",
    "    review = r[\"content\"]\n",
    "    pred, score = get_weighted_lexicon_pred(review, lexicon_dict, threshold=bias_threshold)\n",
    "    y_magnitude.append(pred)\n",
    "    scores.append(score)\n",
    "\n",
    "scores = np.array(scores)\n",
    "\n",
    "token_results_weighted = [\n",
    "    1 if y == y_magnitude[i] else 0\n",
    "    for i, y in enumerate([r[\"sentiment\"] for r in reviews])\n",
    "]\n",
    "token_accuracy_weighted = sum(token_results_weighted) / len(token_results_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "error",
     "timestamp": 1698047779696,
     "user": {
      "displayName": "XUE LI",
      "userId": "02386889076941396043"
     },
     "user_tz": -120
    },
    "id": "9vVk7CvDpyka",
    "outputId": "fb9a09d2-b878-416f-d72c-1c651099c9ed"
   },
   "outputs": [],
   "source": [
    "magnitude_results = [\n",
    "    1 if y == y_magnitude[i] else 0\n",
    "    for i, y in enumerate([r[\"sentiment\"] for r in reviews])\n",
    "]\n",
    "magnitude_accuracy = sum(token_results_weighted) / len(token_results_weighted)\n",
    "print(f\"Accuracy: {magnitude_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9SHoGPfsAHV"
   },
   "source": [
    "#### (Q.1.3) Make a barplot of the two results (0.5pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LgBcYcXsEk3"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar([\"Binary\", \"Weighted\"], [token_accuracy, magnitude_accuracy])\n",
    "ax.set_xlabel(\"Method\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy of Sentiment Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNhS8OCVxMHd"
   },
   "source": [
    "#### (Q1.4) A better threshold (1pt)\n",
    "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
    "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xo7gk1I-omLI"
   },
   "source": [
    "The problem with not accounting with sequence length is that we may have long sequence with a majority of positive or negatives words. These long sequences are outliers that impact the bias threshold we are using to classify the sentiment of the review. To solve the problem, we can normalize the score by the length of the sequence. This way, we can have a better threshold that is not biased by the length of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normed_lexicon_pred(review, lexicon, threshold = 2) -> tuple[str, float]:\n",
    "    score = 0\n",
    "    review_tokens = []\n",
    "    for sentence in review:\n",
    "        review_tokens += [token for token, _ in sentence]\n",
    "    for token in review_tokens:\n",
    "        if token in lexicon:\n",
    "            polarity = lexicon[token][\"polarity\"]\n",
    "\n",
    "            # Transform polarity and magnitude to numerical values\n",
    "            if polarity == \"positive\":\n",
    "                polarity_value = 1\n",
    "            elif polarity == \"negative\":\n",
    "                polarity_value = -1\n",
    "            else:\n",
    "                polarity_value = 0\n",
    "\n",
    "            magnitude = lexicon[token][\"magnitude\"]\n",
    "            if magnitude == \"weaksubj\":\n",
    "                magnitude_value = 1\n",
    "            elif magnitude == \"strongsubj\":\n",
    "                magnitude_value = 2\n",
    "\n",
    "            score += polarity_value * magnitude_value\n",
    "    score /= len(review_tokens)\n",
    "    if score > threshold:\n",
    "        return \"POS\", score\n",
    "    else:\n",
    "        return \"NEG\", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dwt0B8h8aKjr"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def get_length_threshold(reviews, lexicon) -> float:\n",
    "    scores = []\n",
    "    for r in reviews:\n",
    "        review = r[\"content\"]\n",
    "        _, score = get_normed_lexicon_pred(review, lexicon)\n",
    "        scores.append(score)\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    threshold = scores.mean()\n",
    "    return threshold\n",
    "\n",
    "length_threshold = get_length_threshold(reviews, lexicon_dict)\n",
    "print(f\"Length Threshold: {length_threshold}\")\n",
    "\n",
    "y_normalized = []\n",
    "for r in reviews:\n",
    "    review = r[\"content\"]\n",
    "    pred, _ = get_normed_lexicon_pred(review, lexicon_dict, threshold=length_threshold)\n",
    "    y_normalized.append(pred)\n",
    "\n",
    "token_results_normalized = [\n",
    "    1 if y == y_normalized[i] else 0\n",
    "    for i, y in enumerate([r[\"sentiment\"] for r in reviews])\n",
    "]\n",
    "token_accuracy_normalized = sum(token_results_normalized) / len(token_results_normalized)\n",
    "print(f\"Accuracy: {token_accuracy_normalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LibV4nR89BXb"
   },
   "source": [
    "# (2) Naive Bayes (9.5pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnF9adQnuwia"
   },
   "source": [
    "\n",
    "Your second task is to program a simple Machine Learning approach that operates\n",
    "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
    "described by Pang et al. (2002). In this approach, the only features we\n",
    "will consider are the words in the text themselves, without bringing in\n",
    "external sources of information. The BoW model is a popular way of\n",
    "representing texts as vectors, making it\n",
    "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
    "However, the BoW representation is also very crude, since it discards\n",
    "all information related to word order and grammatical structure in the\n",
    "original text—as the name suggests.\n",
    "\n",
    "## Writing your own classifier (4pts)\n",
    "\n",
    "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
    "a reminder, the Naive Bayes classifier works according to the following\n",
    "equation:\n",
    "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
    "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
    "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
    "vector. Remember that we use the log of these probabilities when making\n",
    "a prediction:\n",
    "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
    "\n",
    "You can find more details about Naive Bayes in [Jurafsky &\n",
    "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
    "this helpful\n",
    "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
    "\n",
    "*Note: this section and the next aim to put you in a position to replicate\n",
    "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
    "    will differ from theirs, as they used different data.*\n",
    "\n",
    "**You must write the Naive Bayes training and prediction code from\n",
    "scratch.** You will not be given credit for using off-the-shelf Machine\n",
    "Learning libraries.\n",
    "\n",
    "The data contains the text of the reviews, where each document consists\n",
    "of the sentences in the review, the sentiment of the review and an index\n",
    "(cv) that you will later use for cross-validation. The\n",
    "text has already been tokenised and POS-tagged for you. Your algorithm\n",
    "should read in the text, **lowercase it**, store the words and their\n",
    "frequencies in an appropriate data structure that allows for easy\n",
    "computation of the probabilities used in the Naive Bayes algorithm, and\n",
    "then make predictions for new instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEpyQSBSkb33"
   },
   "source": [
    "#### (Q2.1) Unseen words (1pt)\n",
    "The presence of words in the test dataset that\n",
    "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
    "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes at test time**.  What would be the problem instead with skipping words only for one class in case 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BanFiYYnoxDW"
   },
   "source": [
    "Skipping words for only one class in case 2 can create an imbalance in the data that would lead to biased predictions. This would happen because:\n",
    "\n",
    "1. Naive Bayes sums the log probability of each word in each class. If a word is ignored in only one class, it would result in an inaccurate comparison between both classes.\n",
    "\n",
    "2. Ignoring a word in only one class would then make the classifier biased towards that class. This happens because the log probability would be unfairly lower for the class in which the word is not being ignored, which could increase the chance to misclassify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsZRhaI3WvzC"
   },
   "source": [
    "#### (Q2.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7zaJYGFvIJ3"
   },
   "outputs": [],
   "source": [
    "train_reviews = [review for review in reviews if int(review[\"cv\"]) < 900]\n",
    "test_reviews = [review for review in reviews if int(review[\"cv\"]) >= 900]\n",
    "\n",
    "def create_vocab_and_counts(reviews):\n",
    "    \"\"\" Function to create the BoW vocabulary and count dictionaries for each word / class \"\"\"\n",
    "\n",
    "    vocab = set()\n",
    "    word_counts = {\"positive\": {}, \"negative\": {}}\n",
    "    class_counts = {\"positive\": 0, \"negative\": 0}\n",
    "\n",
    "    for review in reviews:\n",
    "\n",
    "        label = \"positive\" if review[\"sentiment\"] == \"POS\" else \"negative\"\n",
    "        class_counts[label] += 1\n",
    "\n",
    "        for sentence in review[\"content\"]:\n",
    "            for word, _ in sentence:\n",
    "                word = word.lower()\n",
    "                \n",
    "                if word not in word_counts[label]:\n",
    "                    word_counts[label][word] = 0\n",
    "\n",
    "                word_counts[label][word] += 1\n",
    "                vocab.add(word)\n",
    "\n",
    "    total_words_per_class = {\n",
    "        \"positive\": sum(word_counts[\"positive\"].values()), \n",
    "        \"negative\": sum(word_counts[\"negative\"].values())\n",
    "    }\n",
    "\n",
    "    return vocab, word_counts, class_counts, total_words_per_class\n",
    "\n",
    "def calc_log_probs(vocab, word_counts, class_counts, total_words_per_class, review_content, label):\n",
    "    \"\"\" Function to calculate the log probabilities of a given review \"\"\"\n",
    "\n",
    "    log_pfic = 0\n",
    "    log_pc = np.log(class_counts[label] / len(train_reviews))  # log(P(c)) ---> Nc / N (the number of words in class C / total number of documents)\n",
    "\n",
    "    for sentence in review_content:\n",
    "        for word, _ in sentence:\n",
    "            word = word.lower()\n",
    "\n",
    "            if word not in vocab:\n",
    "                continue\n",
    "            elif word_counts[\"positive\"].get(word, 0) == 0 or word_counts[\"negative\"].get(word, 0) == 0:\n",
    "                continue\n",
    "            \n",
    "            log_pfic += np.log(word_counts[label].get(word, 0) / total_words_per_class[label]) # log(P(f_i|c)) ---> Tct / ∑ Tct' (Number of occurrences of word t on class C / Sum of all words on class C)\n",
    "\n",
    "    return log_pc + log_pfic\n",
    "\n",
    "def calc_acc(vocab, word_counts, class_counts, total_words_per_class, reviews):\n",
    "    \"\"\" Function to calculate the classification accuracy of a set of reviews \"\"\"\n",
    "\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for review in reviews:\n",
    "\n",
    "        log_prob_positive = calc_log_probs(vocab, word_counts, class_counts, total_words_per_class, review[\"content\"], \"positive\")\n",
    "        log_prob_negative = calc_log_probs(vocab, word_counts, class_counts, total_words_per_class, review[\"content\"], \"negative\")\n",
    "\n",
    "        true_label = \"positive\" if review[\"sentiment\"] == \"POS\" else \"negative\"\n",
    "        predicted_label = \"positive\" if log_prob_positive > log_prob_negative else \"negative\"\n",
    "\n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    return correct_predictions / len(reviews)\n",
    "\n",
    "vocab, word_counts, class_counts, total_words_per_class = create_vocab_and_counts(train_reviews)\n",
    "accuracy = calc_acc(vocab, word_counts, class_counts, total_words_per_class, test_reviews)\n",
    "print(f\"Classification Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0INK-PBoM6CB"
   },
   "source": [
    "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
    "\n",
    "Simulate this scenario by keeping the positive reviews\n",
    "data unchanged, but only using negative reviews cv000–cv089 for\n",
    "training, and cv900–cv909 for testing. Calculate the classification\n",
    "accuracy, and explain what changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFbcsYlipBAw"
   },
   "source": [
    "On the previous question we managed to obtain an accuracy of 82.5% with a balanced training dataset. When introducing class imbalance to our classifier, the accuracy dropped to only 60%. This happens because Naive Bayes classifiers are expected to have equal representation for both classes.\n",
    "\n",
    "Since we barely trained the classifier on negative reviews, it became less capable of identifying them and generalizing the patterns associated with it. On the hand, the classifier was much more exposed to positive reviews, so it developed a biased tendency towards it which resulted in the loss of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWDkt5ZrrFGp"
   },
   "outputs": [],
   "source": [
    "imbalanced_train_reviews = [review for review in reviews if int(review[\"cv\"]) < 900 and review[\"sentiment\"] == \"POS\"]\n",
    "imbalanced_train_reviews.extend([review for review in reviews if int(review[\"cv\"]) < 90 and review[\"sentiment\"] == \"NEG\"])\n",
    "\n",
    "imbalanced_test_reviews = [review for review in reviews if int(review[\"cv\"]) >= 900 and review[\"sentiment\"] == \"POS\"]\n",
    "imbalanced_test_reviews.extend([review for review in reviews if int(review[\"cv\"]) >= 900 and int(review[\"cv\"]) < 910 and review[\"sentiment\"] == \"NEG\"])\n",
    "\n",
    "vocab, word_counts, class_counts, total_words_per_class = create_vocab_and_counts(imbalanced_train_reviews)\n",
    "accuracy = calc_acc(vocab, word_counts, class_counts, total_words_per_class, imbalanced_test_reviews)\n",
    "print(f\"Classification Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wJzcHX3WUDm"
   },
   "source": [
    "## Smoothing (1pt)\n",
    "\n",
    "As mentioned above, the presence of words in the test dataset that\n",
    "have not been seen during training can cause probabilities in the Naive\n",
    "Bayes classifier to be $0$, thus making that particular test instance\n",
    "undecidable. The standard way to mitigate this effect (as well as to\n",
    "give more clout to rare words) is to use smoothing, in which the\n",
    "probability fraction\n",
    "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$ \n",
    "for a word $w_i$ becomes\n",
    "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBNIcbwUWphC"
   },
   "source": [
    "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
    "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
    "Bayes classifier’s code, and report the accuracy.\n",
    "Use $\\kappa = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g03yflCc9kpW"
   },
   "outputs": [],
   "source": [
    "train_reviews = [review for review in reviews if int(review[\"cv\"]) < 900]\n",
    "test_reviews = [review for review in reviews if int(review[\"cv\"]) >= 900]\n",
    "\n",
    "def calc_log_probs(vocab, word_counts, class_counts, total_words_per_class, review_content, label):\n",
    "\n",
    "    log_pfic = 0\n",
    "    log_pc = np.log(class_counts[label] / len(train_reviews))  # log(P(c))\n",
    "\n",
    "    for sentence in review_content:\n",
    "        for word, _ in sentence:\n",
    "            word = word.lower()\n",
    "\n",
    "            if word not in vocab:\n",
    "                continue\n",
    "            elif word_counts[\"positive\"].get(word, 0) == 0 or word_counts[\"negative\"].get(word, 0) == 0:\n",
    "                continue\n",
    "            \n",
    "            log_pfic += np.log((word_counts[label].get(word, 0) + 1) / ((total_words_per_class[label]) + len(vocab))) # log(P(f_i|c)) with Laplace smoothing\n",
    "\n",
    "    return log_pc + log_pfic\n",
    "\n",
    "vocab, word_counts, class_counts, total_words_per_class = create_vocab_and_counts(train_reviews)\n",
    "accuracy = calc_acc(vocab, word_counts, class_counts, total_words_per_class, test_reviews)\n",
    "print(f\"Classification Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiGcgwba87D5"
   },
   "source": [
    "## Cross-Validation (1.5pts)\n",
    "\n",
    "A serious danger in using Machine Learning on small datasets, with many\n",
    "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
    "suggested by the data” errors. This type of error occurs when we make\n",
    "repeated improvements to our classifiers by playing with features and\n",
    "their processing, but we don’t get a fresh, never-before seen test\n",
    "dataset every time. Thus, we risk developing a classifier that gets better\n",
    "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
    "\n",
    "A simple method to guard against Type III errors is to use\n",
    "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
    "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
    "time holding out one of the folds for testing, training our classifier\n",
    "on the remaining N - 1 data folds, and reporting performance on the\n",
    "held-out fold. We can use different strategies for dividing the data:\n",
    "\n",
    "-   Consecutive splitting:\n",
    "  - cv000–cv099 = Split 1\n",
    "  - cv100–cv199 = Split 2\n",
    "  - etc.\n",
    "  \n",
    "-   Round-robin splitting (mod 10):\n",
    "  - cv000, cv010, cv020, … = Split 1\n",
    "  - cv001, cv011, cv021, … = Split 2\n",
    "  - etc.\n",
    "\n",
    "-   Random sampling/splitting\n",
    "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OeLcbSauGtR"
   },
   "source": [
    "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KeCGPa7Nuzx"
   },
   "outputs": [],
   "source": [
    "fold_accuracies = []\n",
    "\n",
    "for fold in range(10):\n",
    "    \n",
    "    train_reviews = [review for review in reviews if int(review[\"cv\"]) % 10 != fold]\n",
    "    test_reviews = [review for review in reviews if int(review[\"cv\"]) % 10 == fold]\n",
    "\n",
    "    vocab, word_counts, class_counts, total_words_per_class = create_vocab_and_counts(train_reviews)\n",
    "    accuracy = calc_acc(vocab, word_counts, class_counts, total_words_per_class, test_reviews)\n",
    "    fold_accuracies.append(accuracy)\n",
    "\n",
    "global_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"Avg. Accuracy: {global_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otdlsDXBNyOa"
   },
   "source": [
    "#### (Q2.6) Report the variance of the 10 accuracy scores. (0.5pt)\n",
    "\n",
    "**Please report all future results using 10-fold cross-validation now\n",
    "(unless told to use the held-out test set).** Note: you're not allowed to use a library for computing the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoBQm1KuNzNR"
   },
   "outputs": [],
   "source": [
    "variance = sum((accuracy - global_accuracy) ** 2 for accuracy in fold_accuracies) / len(fold_accuracies)\n",
    "print(f\"Variance of Accuracy: {variance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6A2zX9_BRKm"
   },
   "source": [
    "## Features, overfitting, and the curse of dimensionality\n",
    "\n",
    "In the Bag-of-Words model, ideally we would like each distinct word in\n",
    "the text to be mapped to its own dimension in the output vector\n",
    "representation. However, real world text is messy, and we need to decide\n",
    "on what we consider to be a word. For example, is “`word`\" different\n",
    "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
    "definition, and the number of features explodes, while our algorithm\n",
    "fails to learn anything generalisable. Too lax, and we risk destroying\n",
    "our learning signal. In the following section, you will learn about\n",
    "confronting the feature sparsity and the overfitting problems as they\n",
    "occur in NLP classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKK8FNt8VtcZ"
   },
   "source": [
    "### Stemming (1.5pts)\n",
    "\n",
    "To make your algorithm more robust, use stemming and hash different inflections of a word to the same feature in the BoW vector space. Please use the [Porter stemming\n",
    "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NxtCul1IrBi_"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def create_vocab_and_counts(reviews):\n",
    "\n",
    "    vocab = set()\n",
    "    word_counts = {\"positive\": {}, \"negative\": {}}\n",
    "    class_counts = {\"positive\": 0, \"negative\": 0}\n",
    "\n",
    "    for review in reviews:\n",
    "\n",
    "        label = \"positive\" if review[\"sentiment\"] == \"POS\" else \"negative\"\n",
    "        class_counts[label] += 1\n",
    "\n",
    "        for sentence in review[\"content\"]:\n",
    "            for word, _ in sentence:\n",
    "                word = stemmer.stem(word.lower())\n",
    "                \n",
    "                if word not in word_counts[label]:\n",
    "                    word_counts[label][word] = 0\n",
    "\n",
    "                word_counts[label][word] += 1\n",
    "                vocab.add(word)\n",
    "\n",
    "    total_words_per_class = {\n",
    "        \"positive\": sum(word_counts[\"positive\"].values()), \n",
    "        \"negative\": sum(word_counts[\"negative\"].values())\n",
    "    }\n",
    "\n",
    "    return vocab, word_counts, class_counts, total_words_per_class\n",
    "\n",
    "def calc_log_probs(vocab, word_counts, class_counts, total_words_per_class, review_content, label):\n",
    "\n",
    "    log_pfic = 0\n",
    "    log_pc = np.log(class_counts[label] / len(train_reviews))  # log(P(c))\n",
    "\n",
    "    for sentence in review_content:\n",
    "        for word, _ in sentence:\n",
    "            word = stemmer.stem(word.lower())\n",
    "\n",
    "            if word not in vocab:\n",
    "                continue\n",
    "            elif word_counts[\"positive\"].get(word, 0) == 0 or word_counts[\"negative\"].get(word, 0) == 0:\n",
    "                continue\n",
    "            \n",
    "            log_pfic += np.log((word_counts[label].get(word, 0) + 1) / ((total_words_per_class[label]) + len(vocab))) # log(P(f_i|c)) with Laplace smoothing\n",
    "\n",
    "    return log_pc + log_pfic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SrJ1BeLXTnk"
   },
   "source": [
    "#### (Q2.7): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
    "Use cross-validation to evaluate the classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYqKBOiIrInT"
   },
   "outputs": [],
   "source": [
    "fold_accuracies = []\n",
    "\n",
    "for fold in range(10):\n",
    "    \n",
    "    train_reviews = [review for review in reviews if int(review[\"cv\"]) % 10 != fold]\n",
    "    test_reviews = [review for review in reviews if int(review[\"cv\"]) % 10 == fold]\n",
    "\n",
    "    vocab, word_counts, class_counts, total_words_per_class = create_vocab_and_counts(train_reviews)\n",
    "    accuracy = calc_acc(vocab, word_counts, class_counts, total_words_per_class, test_reviews)\n",
    "    fold_accuracies.append(accuracy)\n",
    "\n",
    "global_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"Avg. Accuracy: {global_accuracy:.3f}\")\n",
    "\n",
    "variance = sum((accuracy - global_accuracy) ** 2 for accuracy in fold_accuracies) / len(fold_accuracies)\n",
    "print(f\"Variance of Accuracy: {variance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkDHVq_1XUVP"
   },
   "source": [
    "#### (Q2.8) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
    "Give actual numbers. You can use the held-out training set to determine these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MA3vee5-rJyy"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "train_reviews = [review for review in reviews if int(review[\"cv\"]) < 900]\n",
    "\n",
    "vocab_w_stem = set()\n",
    "vocab_wo_stem = set()\n",
    "\n",
    "for review in train_reviews:\n",
    "    for sentence in review[\"content\"]:\n",
    "        for word, _ in sentence:\n",
    "\n",
    "            word_clean = word.lower()\n",
    "            word_stem = stemmer.stem(word.lower())\n",
    "\n",
    "            vocab_w_stem.add(word_stem)\n",
    "            vocab_wo_stem.add(word_clean)\n",
    "\n",
    "print(f\"Vocabulary Size Without Stemming: {len(vocab_wo_stem)}\")\n",
    "print(f\"Vocabulary Size With Stemming: {len(vocab_w_stem)}\")\n",
    "print(f\"Difference in Vocabulary Size Due to Stemming: {len(vocab_wo_stem) - len(vocab_w_stem)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoazfxbNV5Lq"
   },
   "source": [
    "### N-grams (1.5pts)\n",
    "\n",
    "A simple way of retaining some of the word\n",
    "order information when using bag-of-words representations is to use **n-gram** features.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHjy3I7-qWiu"
   },
   "source": [
    "#### (Q2.9) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
    "Report accuracy and compare it with that of the approaches you have previously implemented. You are allowed to use NLTK to build n-grams from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYuKMTOpq9jz"
   },
   "outputs": [],
   "source": [
    "train_reviews = [review for review in reviews if int(review[\"cv\"]) < 900]\n",
    "test_reviews = [review for review in reviews if int(review[\"cv\"]) >= 900]\n",
    "\n",
    "def get_ngrams_list(review, features_num):\n",
    "\n",
    "    ngrams_list = []\n",
    "\n",
    "    for sentence in review:\n",
    "        tokens = [stemmer.stem(token.lower()) for token, _ in sentence]\n",
    "        for n in range(1, features_num + 1):\n",
    "            ngrams_list.append(ngrams(tokens, n))\n",
    "\n",
    "    return ngrams_list\n",
    "\n",
    "def create_vocab_and_counts(reviews, features_num):\n",
    "\n",
    "    vocab = set()\n",
    "    word_counts = {\"positive\": {}, \"negative\": {}}\n",
    "    class_counts = {\"positive\": 0, \"negative\": 0}\n",
    "\n",
    "    for review in reviews:\n",
    "\n",
    "        label = \"positive\" if review[\"sentiment\"] == \"POS\" else \"negative\"\n",
    "        class_counts[label] += 1\n",
    "\n",
    "        ngrams_list = get_ngrams_list(review[\"content\"], features_num)\n",
    "\n",
    "        for ngrams in ngrams_list:\n",
    "            for ngram in ngrams:\n",
    "                ngram = ' '.join(ngram)\n",
    "                \n",
    "                if ngram not in word_counts[label]:\n",
    "                    word_counts[label][ngram] = 0\n",
    "\n",
    "                word_counts[label][ngram] += 1\n",
    "                vocab.add(ngram)\n",
    "\n",
    "    total_words_per_class = {\n",
    "        \"positive\": sum(word_counts[\"positive\"].values()), \n",
    "        \"negative\": sum(word_counts[\"negative\"].values())\n",
    "    }\n",
    "\n",
    "    return vocab, word_counts, class_counts, total_words_per_class\n",
    "\n",
    "def calc_log_probs(vocab, word_counts, class_counts, total_words_per_class, review, label, features_num):\n",
    "\n",
    "    log_pfic = 0\n",
    "    log_pc = np.log(class_counts[label] / len(train_reviews))  # log(P(c))\n",
    "\n",
    "    ngrams_list = get_ngrams_list(review, features_num)\n",
    "\n",
    "    for ngrams in ngrams_list:\n",
    "        for ngram in ngrams:\n",
    "            ngram = ' '.join(ngram)\n",
    "\n",
    "            if ngram not in vocab:\n",
    "                continue\n",
    "            elif word_counts[\"positive\"].get(ngram, 0) == 0 or word_counts[\"negative\"].get(ngram, 0) == 0:\n",
    "                continue\n",
    "            \n",
    "            log_pfic += np.log((word_counts[label].get(ngram, 0) + 1) / ((total_words_per_class[label]) + len(vocab))) # log(P(f_i|c)) with Laplace smoothing\n",
    "\n",
    "    return log_pc + log_pfic\n",
    "\n",
    "def calc_acc(vocab, word_counts, class_counts, total_words_per_class, reviews, features_num):\n",
    "\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for review in reviews:\n",
    "\n",
    "        log_prob_positive = calc_log_probs(vocab, word_counts, class_counts, total_words_per_class, review[\"content\"], \"positive\", features_num)\n",
    "        log_prob_negative = calc_log_probs(vocab, word_counts, class_counts, total_words_per_class, review[\"content\"], \"negative\", features_num)\n",
    "\n",
    "        true_label = \"positive\" if review[\"sentiment\"] == \"POS\" else \"negative\"\n",
    "        predicted_label = \"positive\" if log_prob_positive > log_prob_negative else \"negative\"\n",
    "\n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    return correct_predictions / len(reviews)\n",
    "\n",
    "for num_words in range(2, 4):\n",
    "\n",
    "    vocab, word_counts, class_counts, total_words_per_class = create_vocab_and_counts(train_reviews, num_words)\n",
    "    accuracy = calc_acc(vocab, word_counts, class_counts, total_words_per_class, test_reviews, num_words)\n",
    "    print(f\"Classification Accuracy using {num_words}-gram features: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVrGGArkrWoL"
   },
   "source": [
    "\n",
    "#### Q2.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
    "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How do the number of features increase in the held-out training set (compared to Q2.8)? Do you expect this rate of increase to continue for (much) larger n-grams?\n",
    "\n",
    "Use the held-out training set once again for this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEGZ9SV8pPaa"
   },
   "source": [
    "The number of features would change depending on the n-grams used for each case.\n",
    "\n",
    "- For unigrams, each word is a feature so the number of features grows linearly with the vocabulary size.\n",
    "- For Bigrams, each two words can form a new feature in the vocabulary, so it would grow quadracticly in size.\n",
    "- For Trigrams, any three words form a new feature, so the size of the vocabulary grows cubicly.\n",
    "\n",
    "We can showcase these ideas with the obtained results, since we can see that the number of features grows really fast, somewhat exponentially. They may not strictly follow a quadratic or cubic increase because of repeated of occurences of certain sequences, but for larger n-grams, the feature count would continue to increase rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_z8sAJeUrdtM"
   },
   "outputs": [],
   "source": [
    "train_reviews = [review for review in reviews if int(review[\"cv\"]) < 900]\n",
    "\n",
    "def get_ngrams_set(review, features_num):\n",
    "\n",
    "    ngrams_list = set()\n",
    "\n",
    "    for sentence in review:\n",
    "        tokens = [stemmer.stem(token.lower()) for token, _ in sentence]\n",
    "        for n in range(1, features_num + 1):\n",
    "            ngrams_list.update(ngrams(tokens, n))\n",
    "\n",
    "    return list(ngrams_list)\n",
    "\n",
    "unigrams = set()\n",
    "uni_bigrams = set()\n",
    "uni_bi_trigrams = set()\n",
    "\n",
    "for review in train_reviews:\n",
    "    unigrams.update(get_ngrams_set(review[\"content\"], 1))\n",
    "    uni_bigrams.update(get_ngrams_set(review[\"content\"], 2))\n",
    "    uni_bi_trigrams.update(get_ngrams_set(review[\"content\"], 3))\n",
    "\n",
    "print(f\"Number of features with 1-gram: {len(unigrams)}\")\n",
    "print(f\"Number of features with 1-gram + 2-gram: {len(uni_bigrams)}\")\n",
    "print(f\"Number of features with 1-gram + 2-gram + 3-gram: {len(uni_bi_trigrams)}\")\n",
    "\n",
    "print(f\"Difference from 1-gram to 2-gram: {len(uni_bigrams) - len(unigrams)}\")\n",
    "print(f\"Difference from 2-gram to 3-gram: {len(uni_bi_trigrams) - len(uni_bigrams)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHWKDL3YV6vh"
   },
   "source": [
    "# (3) Support Vector Machines (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJSYhcVaoJGt"
   },
   "source": [
    "Though simple to understand, implement, and debug, one\n",
    "major problem with the Naive Bayes classifier is that its performance\n",
    "deteriorates (becomes skewed) when it is being used with features which\n",
    "are not independent (i.e., are correlated). Another popular classifier\n",
    "that doesn’t scale as well to big data, and is not as simple to debug as\n",
    "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
    "Vector Machine (SVM) classifier.\n",
    "\n",
    "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
    "Other sources for learning SVM:\n",
    "* http://web.mit.edu/zoya/www/SVM.pdf\n",
    "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
    "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Use the scikit-learn implementation of\n",
    "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LnzNtQBV8gr"
   },
   "source": [
    "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
    "\n",
    "Train an SVM classifier (sklearn.svm.LinearSVC) using the features collected for Naive Bayes. Compare the\n",
    "classification performance of the SVM classifier to that of the Naive\n",
    "Bayes classifier with smoothing.\n",
    "Use cross-validation to evaluate the performance of the classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "JBscui8Mvoz0"
   },
   "outputs": [],
   "source": [
    "def get_svm_features(reviews, vocab, features_num=1):\n",
    "    vocab_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    X_svm = np.zeros((len(reviews), len(vocab)))\n",
    "    for review_idx, review in enumerate(reviews):\n",
    "        ngrams_list = get_ngrams_list(review[\"content\"], features_num)\n",
    "        for review_ngrams in ngrams_list:\n",
    "            for ngram in review_ngrams:\n",
    "                ngram = ' '.join(ngram)\n",
    "                if ngram in vocab_to_idx:\n",
    "                    X_svm[review_idx, vocab_to_idx[ngram]] += 1\n",
    "    return X_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for fold in tqdm(range(10)):\n",
    "    train_reviews = [review for review in reviews if int(review[\"cv\"]) % 10 != fold]\n",
    "    test_reviews = [review for review in reviews if int(review[\"cv\"]) % 10 == fold]\n",
    "\n",
    "    vocab, word_counts, class_counts, total_words_per_class = create_vocab_and_counts(train_reviews, features_num=1)\n",
    "    X_train = get_svm_features(train_reviews, vocab)\n",
    "    X_test = get_svm_features(test_reviews, vocab)\n",
    "\n",
    "    y_train = [1 if review[\"sentiment\"] == \"POS\" else 0 for review in train_reviews]\n",
    "    y_test = [1 if review[\"sentiment\"] == \"POS\" else 0 for review in test_reviews]\n",
    "\n",
    "    svc = sk.svm.SVC(kernel=\"linear\")\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred_train = svc.predict(X_train)\n",
    "    y_pred_test = svc.predict(X_test)\n",
    "\n",
    "    train_accuracy = sk.metrics.accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = sk.metrics.accuracy_score(y_test, y_pred_test)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "global_train_accuracy = np.mean(train_accuracies)\n",
    "train_variance = sum((accuracy - global_accuracy) ** 2 for accuracy in fold_accuracies) / len(fold_accuracies)\n",
    "print(f\"Train Accuracy: {global_train_accuracy:.3f} (Variance: {train_variance:.3f})\")\n",
    "\n",
    "global_test_accuracy = np.mean(test_accuracies)\n",
    "test_variance = sum((accuracy - global_accuracy) ** 2 for accuracy in fold_accuracies) / len(fold_accuracies)\n",
    "print(f\"Test Accuracy: {global_test_accuracy:.3f} (Variance: {test_variance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifXVWcK0V9qY"
   },
   "source": [
    "### POS disambiguation (2pts)\n",
    "\n",
    "Now add in part-of-speech features. You will find the\n",
    "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
    "replicate the results obtained by Pang et al. (2002).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA3I82o4oWGu"
   },
   "source": [
    "####(Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "NOvjYe-t2Br6"
   },
   "outputs": [],
   "source": [
    "def get_svm_pos_features(reviews, token_vocab, pos_vocab, features_num=1):\n",
    "    vocab_to_idx = {word: idx for idx, word in enumerate(token_vocab)}\n",
    "    pos_to_idx = {pos: idx + len(token_vocab) for idx, pos in enumerate(pos_vocab)}\n",
    "    X_svm = np.zeros((len(reviews), len(token_vocab) + len(pos_vocab)))\n",
    "    for review_idx, review in enumerate(reviews):\n",
    "        ngrams_list = get_ngrams_list(review[\"content\"], features_num)\n",
    "\n",
    "        # Populate token features\n",
    "        for review_ngrams in ngrams_list:\n",
    "            for ngram in review_ngrams:\n",
    "                ngram = ' '.join(ngram)\n",
    "                if ngram in vocab_to_idx:\n",
    "                    X_svm[review_idx, vocab_to_idx[ngram]] += 1\n",
    "\n",
    "        # Populate POS features\n",
    "        for sentence in review[\"content\"]:\n",
    "            for _, pos in sentence:\n",
    "                if pos in pos_to_idx:\n",
    "                    X_svm[review_idx, pos_to_idx[pos]] = 1\n",
    "    return X_svm\n",
    "\n",
    "def get_pos_vocab(reviews):\n",
    "    vocab = set()\n",
    "    for review in reviews:\n",
    "        for sentence in review[\"content\"]:\n",
    "            for _, pos in sentence:\n",
    "                vocab.add(pos)\n",
    "    return vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for fold in tqdm(range(10)):\n",
    "    train_reviews = [review for review in reviews if int(review[\"cv\"]) % 10 != fold]\n",
    "    test_reviews = [review for review in reviews if int(review[\"cv\"]) % 10 == fold]\n",
    "\n",
    "    token_vocab, word_counts, class_counts, total_words_per_class = create_vocab_and_counts(train_reviews, features_num=1)\n",
    "    pos_vocab = get_pos_vocab(train_reviews)\n",
    "    X_train = get_svm_pos_features(train_reviews, token_vocab, pos_vocab)\n",
    "    X_test = get_svm_pos_features(test_reviews, token_vocab, pos_vocab)\n",
    "\n",
    "    y_train = [1 if review[\"sentiment\"] == \"POS\" else 0 for review in train_reviews]\n",
    "    y_test = [1 if review[\"sentiment\"] == \"POS\" else 0 for review in test_reviews]\n",
    "\n",
    "    svc = sk.svm.SVC(kernel=\"linear\")\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred_train = svc.predict(X_train)\n",
    "    y_pred_test = svc.predict(X_test)\n",
    "\n",
    "    train_accuracy = sk.metrics.accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = sk.metrics.accuracy_score(y_test, y_pred_test)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "global_train_accuracy = np.mean(train_accuracies)\n",
    "train_variance = sum((accuracy - global_accuracy) ** 2 for accuracy in fold_accuracies) / len(fold_accuracies)\n",
    "print(f\"Train Accuracy: {global_train_accuracy:.3f} (Variance: {train_variance:.3f})\")\n",
    "\n",
    "global_test_accuracy = np.mean(test_accuracies)\n",
    "test_variance = sum((accuracy - global_accuracy) ** 2 for accuracy in fold_accuracies) / len(fold_accuracies)\n",
    "print(f\"Test Accuracy: {global_test_accuracy:.3f} (Variance: {test_variance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0dt_oQupUNe"
   },
   "source": [
    "The results show that including the POS tags does not help with the classification performance. In fact, we get an accuracy that is 0.2% lower when we include the POS tags. These results are consistent with the results from Pang et al. (2002), where they show that including the POS tags lowers the accuracy of the SVM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su-3w87eMW0w"
   },
   "source": [
    "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "CCUPlPozCYUX"
   },
   "outputs": [],
   "source": [
    "def get_ngrams_list_filtered(review, features_num):\n",
    "    ngrams_list = []\n",
    "    allowed_pos = set(\n",
    "        [\n",
    "            \"NNS\",\n",
    "            \"NN\",\n",
    "            \"NNP\",\n",
    "            \"NNPS\",\n",
    "            \"VB\",\n",
    "            \"VBD\",\n",
    "            \"VBG\",\n",
    "            \"VBN\",\n",
    "            \"VBP\",\n",
    "            \"VBZ\",\n",
    "            \"JJ\",\n",
    "            \"JJR\",\n",
    "            \"JJS\",\n",
    "            \"RB\",\n",
    "            \"RBR\",\n",
    "            \"RBS\",\n",
    "        ]\n",
    "    )\n",
    "    for sentence in review:\n",
    "        tokens = [\n",
    "            stemmer.stem(token.lower()) for token, pos in sentence if pos in allowed_pos\n",
    "        ]\n",
    "        for n in range(1, features_num + 1):\n",
    "            ngrams_list.append(ngrams(tokens, n))\n",
    "\n",
    "    return ngrams_list\n",
    "\n",
    "def get_svm_filtered_features(reviews, token_vocab, pos_vocab, features_num=1):\n",
    "    vocab_to_idx = {word: idx for idx, word in enumerate(token_vocab)}\n",
    "    pos_to_idx = {pos: idx + len(token_vocab) for idx, pos in enumerate(pos_vocab)}\n",
    "    X_svm = np.zeros((len(reviews), len(token_vocab) + len(pos_vocab)))\n",
    "    for review_idx, review in enumerate(reviews):\n",
    "        ngrams_list = get_ngrams_list_filtered(review[\"content\"], features_num)\n",
    "\n",
    "        # Populate token features\n",
    "        for review_ngrams in ngrams_list:\n",
    "            for ngram in review_ngrams:\n",
    "                ngram = ' '.join(ngram)\n",
    "                if ngram in vocab_to_idx:\n",
    "                    X_svm[review_idx, vocab_to_idx[ngram]] += 1\n",
    "\n",
    "        # Populate POS features\n",
    "        for sentence in review[\"content\"]:\n",
    "            for _, pos in sentence:\n",
    "                if pos in pos_to_idx:\n",
    "                    X_svm[review_idx, pos_to_idx[pos]] = 1\n",
    "    return X_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for fold in tqdm(range(10)):\n",
    "    train_reviews = [review for review in reviews if int(review[\"cv\"]) % 10 != fold]\n",
    "    test_reviews = [review for review in reviews if int(review[\"cv\"]) % 10 == fold]\n",
    "\n",
    "    token_vocab, word_counts, class_counts, total_words_per_class = create_vocab_and_counts(train_reviews, features_num=1)\n",
    "    pos_vocab = get_pos_vocab(train_reviews)\n",
    "    X_train = get_svm_filtered_features(train_reviews, token_vocab, pos_vocab)\n",
    "    X_test = get_svm_filtered_features(test_reviews, token_vocab, pos_vocab)\n",
    "\n",
    "    y_train = [1 if review[\"sentiment\"] == \"POS\" else 0 for review in train_reviews]\n",
    "    y_test = [1 if review[\"sentiment\"] == \"POS\" else 0 for review in test_reviews]\n",
    "\n",
    "    svc = sk.svm.SVC(kernel=\"linear\")\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred_train = svc.predict(X_train)\n",
    "    y_pred_test = svc.predict(X_test)\n",
    "\n",
    "    train_accuracy = sk.metrics.accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = sk.metrics.accuracy_score(y_test, y_pred_test)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "global_train_accuracy = np.mean(train_accuracies)\n",
    "train_variance = sum((accuracy - global_accuracy) ** 2 for accuracy in fold_accuracies) / len(fold_accuracies)\n",
    "print(f\"Train Accuracy: {global_train_accuracy:.3f} (Variance: {train_variance:.3f})\")\n",
    "\n",
    "global_test_accuracy = np.mean(test_accuracies)\n",
    "test_variance = sum((accuracy - global_accuracy) ** 2 for accuracy in fold_accuracies) / len(fold_accuracies)\n",
    "print(f\"Test Accuracy: {global_test_accuracy:.3f} (Variance: {test_variance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaxCVrs8pWSp"
   },
   "source": [
    "Removing the closed-class words from the data helps with the classification performance and we get an improvement of 0.7% in accuracy. Closed-class words are detrimental to the classifier because they are not the POS that carry most of the sentiment information. That is why, by removing the closed-class words, we avoid overfitting on the closed-class words and focus on the POS that carry most of the sentiment information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfwqOciAl2No"
   },
   "source": [
    "# (4) Discussion (max. 500 words). (5pts)\n",
    "\n",
    "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
    "Why is this important? What are the limitations of these features and techniques?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYuse5WLmekZ"
   },
   "source": [
    "Based on our experiments, we can compare and assess the effectiveness of different features and techniques in sentiment analysis by observing their impact on the classifier's accuracy.\n",
    "\n",
    "For the lexicon-based approach, we initially calculated scores using only sentiment labels for each word in a review and applied a threshold, achieving 68% accuracy. However, this approach was limited by the dataset’s polarity bias, with approximately eight more positive words than negative ones per review. To take this into consideration, we incorporated magnitude values for sentiment intensity and optimized the threshold, but these adjustments didn’t yield a significant accuracy improvement. Ultimately, despite its simplicity, the lexicon-based approach managed to capture sentiment on a moderate level. It is a straightforward method and can capture basic sentiment information which still lacks context sensitivity, so it is not optimal to handle more complex emotional expressions.\n",
    "\n",
    "For the Naive Bayes classifier, we used a Bag-of-Words approach, resulting in a much higher accuracy of 83.5% with Laplace smoothing, highlighting that statistical models outperforms lexicon-based methods. Naive Bayes treats words as independent features, which works well for simple datasets. This independence assumption, however, prevents it from considering word sequences and context, limiting its effectiveness with complex linguistic structures. By later introducing N-grams (unigrams, bigrams, and trigrams) as features, the model captured some of these word dependencies, leading to a slight accuracy improvement (84% with unigrams + bigrams + trigrams).\n",
    "\n",
    "The Support Vector Machine model performed similarly to Naive Bayes but demonstrated the value of syntactic features such as part-of-speech tags. By removing closed-class words and adding only POS features like nouns, verbs, adjectives, and adverbs, the SVM could focus more effectively on sentiment-heavy words, improving accuracy to 84.2%. This suggests that filtering for specific syntactic features can help the model detect more significant sentiment-bearing elements.\n",
    "\n",
    "Our experiments ultimately demonstrated the importance of feature encoding for any text-recognition classifier. Lexical features, like sentiment labels and magnitudes, encode basic polarity but often lack context. Statistical features, such as N-grams, capture word associations and sequences, allowing for a more nuanced sentiment representation, while syntactic features add a layer that highlights grammatical structure and the types of words most likely to express sentiment. These differences matter because sentiment relies not only on individual words but also on their relationships and grammatical roles. Effective sentiment analysis requires a combination of these features to capture more complex ideas. \n",
    "\n",
    "Each approach, however, has its own limitations. Lexicon-based models are static and context-insensitive. Naive Bayes and SVM perform well under simple assumptions but may struggle with complex structures or language. Additionally, increasing N-gram features can lead to overfitting and higher computational costs, especially in large datasets. Overall, combining lexicon, statistical, and syntactic features appears to be the most effective strategy to the best sentiment classifier possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwaKwfWQhRk_"
   },
   "source": [
    "# Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOUeaET5ijk-"
   },
   "outputs": [],
   "source": [
    "# Carlos Miguel Patiño - 15485250\n",
    "# Jose Luis Garcia - 15388867"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A9K-H6Tii3X"
   },
   "source": [
    "**That's it!**\n",
    "\n",
    "- Check if you answered all questions fully and correctly.\n",
    "- Download your completed notebook using `File -> Download .ipynb`\n",
    "- Check if your answers are all included in the file you submit.\n",
    "- Submit your .ipynb file via *Canvas*. One submission per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHslatYAKBrF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
